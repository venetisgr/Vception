{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vception.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJbGO3O8IzozU/9zYX86hB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venetisgr/Vception/blob/master/vception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhCNtP11Xmaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os \n",
        "\n",
        "base = \" \" #string containing the location of our data\n",
        "classes = os.listdir(base) #stores the names/location of each class\n",
        "no_classes = len(classes)\n",
        "print(no_classes)\n",
        "#print(classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-OMFgLRXnvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = {} #Here we will store the full path of each sample\n",
        "IDs = []\n",
        "i = 0 \n",
        "\n",
        "for c in classes: #iterates through the available classes \n",
        "    c_images = os.listdir(base+c)\n",
        "    \n",
        "    for image in c_images:#iterates through the specific class folder\n",
        "        IDs.append(c+\"/\"+image)  #'A.J._Buckley/0.npy', 'A.J._Buckley/1.npy'\n",
        "        labels[c+\"/\"+image] = i  #{'A.J._Buckley/0.npy': 0, 'A.J._Buckley/1.npy': 0}\n",
        "    i+=1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDR-WbKCXnya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random #validation split\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "random.seed(7)\n",
        "random.shuffle(IDs)\n",
        "\n",
        "split = int(0.85 * len(IDs))\n",
        "\n",
        "train_ids = IDs[0:split]\n",
        "\n",
        "valid_ids = IDs[split:]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWz_gP1fYe2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_ids[1])\n",
        "print(labels[train_ids[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2usSD5pYiSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We must know in order to break the loop\n",
        "\n",
        "print(no_classes)\n",
        "print(len(IDs))\n",
        "print(len(train_ids))\n",
        "print(len(valid_ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5fDCx60YCF0",
        "colab_type": "text"
      },
      "source": [
        "We need to create a generator that will feed our neural network the data in batches. Furthermore, it will be able to normalize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxinztKEXn07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, IDs, labels, base, batch_size=32, dim=(64,601), n_channels=1,\n",
        "                 n_classes=1211, shuffle=True, no_samples=None):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.IDs = IDs\n",
        "        self.labels = labels\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.no_samples=no_samples\n",
        "        self.on_epoch_end()\n",
        "        self.base = base\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(self.no_samples / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels), dtype=np.float32)\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            temp = np.load(self.base + ID )\n",
        "            temp = temp.astype(np.float32)\n",
        "            #mean and std normalization\n",
        "            mean = np.mean(temp, axis=1)\n",
        "            std = np.std(temp, axis=1)\n",
        "            temp = (temp-mean[0])/std[0]\n",
        "            \n",
        "            X[i,] = np.expand_dims(temp, axis=2)\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "        \n",
        "       \n",
        "        \n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guv5pIN0Xn3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example of one hot encoding\n",
        "keras.utils.to_categorical(5, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Gbr_isgXn5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We initialize our generators\n",
        "\n",
        "train_generator = DataGenerator(train_ids, labels, base, batch_size = batch_size, n_classes=no_classes, no_samples=len(train_ids))\n",
        "valid_generator = DataGenerator(valid_ids, labels, base, batch_size = batch_size, n_classes=no_classes, no_samples=len(valid_ids))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjagjOvQXn7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x , y in train_generator:\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R31HeRoBXn9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(type(x[0,0,0,0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w99vUAz8Xn_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dropout, Input, Conv3D, PReLU,SeparableConv2D, add,GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import layers\n",
        "\n",
        "import os\n",
        "from glob import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkF3-RM9Zp2Z",
        "colab_type": "text"
      },
      "source": [
        "#Model Architecture  Vception - [3,5,7]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NT1JSekTXoB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Architecture  Vception - [3,5,7]\n",
        "\n",
        "channel_axis = -1\n",
        "\n",
        "inpt = layers.Input(shape=(64, 601, 1)) #  mfcc,samples\n",
        "\n",
        "#\n",
        "x = Conv2D(32, (3, 3),strides=(2, 2),use_bias=False)(inpt)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "\n",
        "#\n",
        "x = Conv2D(64, (3, 3),use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "\n",
        "##\n",
        "residual = Conv2D(128, (1, 1),strides=(2, 2),padding='same',use_bias=False)(x)\n",
        "residual = BatchNormalization(axis=channel_axis)(residual)\n",
        "#no relu\n",
        "\n",
        "#\n",
        "x = SeparableConv2D(128, (3, 3), padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = SeparableConv2D(128, (3, 3), padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "#no relu\n",
        "x = MaxPooling2D((3, 3),strides=(2, 2),padding='same')(x)\n",
        "\n",
        "#ADD\n",
        "x = add([x, residual])\n",
        "\n",
        "##\n",
        "residual = Conv2D(256, (1, 1),strides=(2, 2),padding='same',use_bias=False)(x)\n",
        "residual = BatchNormalization(axis=channel_axis)(residual)\n",
        "#no relu\n",
        "\n",
        "#\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = SeparableConv2D(256, (3, 3), padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = SeparableConv2D(256, (3, 3), padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "#no relu\n",
        "x = MaxPooling2D((3, 3),strides=(2, 2),padding='same')(x)\n",
        "\n",
        "#ADD\n",
        "x = add([x, residual])\n",
        "\n",
        "##\n",
        "residual = Conv2D(728, (1, 1),strides=(2, 2),padding='same',use_bias=False)(x)\n",
        "residual = BatchNormalization(axis=channel_axis)(residual)\n",
        "#no relu\n",
        "\n",
        "#\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = SeparableConv2D(728, (3, 3), padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = SeparableConv2D(728, (3, 3), padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "#no relu\n",
        "x = MaxPooling2D((3, 3),strides=(2, 2),padding='same')(x)\n",
        "\n",
        "#ADD\n",
        "x = add([x, residual])\n",
        "\n",
        "##############################################################################\n",
        "\n",
        "for i in range(8): #number of main block multiples\n",
        "    \n",
        "    residual = x\n",
        "    \n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    \n",
        "    #x1\n",
        "    x1 = SeparableConv2D(728, (3, 3), padding='same',use_bias=False)(x)\n",
        "    x1 = BatchNormalization(axis=channel_axis)(x1)\n",
        "    x1 = Activation('relu')(x1)\n",
        "\n",
        "    x1 = SeparableConv2D(728, (3, 3), padding='same',use_bias=False)(x1)\n",
        "    x1 = BatchNormalization(axis=channel_axis)(x1)\n",
        "    x1= Activation('relu')(x1)\n",
        "    \n",
        "    x1 = SeparableConv2D(728, (3, 3), padding='same',use_bias=False)(x1)\n",
        "    x1 = BatchNormalization(axis=channel_axis)(x1)\n",
        "    #no relu\n",
        "    #no maxpool\n",
        "\n",
        "    #x2\n",
        "    x2 = SeparableConv2D(728, (5, 5), padding='same',use_bias=False)(x)\n",
        "    x2 = BatchNormalization(axis=channel_axis)(x2)\n",
        "    x2 = Activation('relu')(x2)\n",
        "\n",
        "    x2 = SeparableConv2D(728, (5, 5), padding='same',use_bias=False)(x2)\n",
        "    x2 = BatchNormalization(axis=channel_axis)(x2)\n",
        "    x2= Activation('relu')(x2)\n",
        "    \n",
        "    x2 = SeparableConv2D(728, (5, 5), padding='same',use_bias=False)(x2)\n",
        "    x2 = BatchNormalization(axis=channel_axis)(x2)\n",
        "    #no relu\n",
        "    #no maxpool\n",
        "    \n",
        "    #x3\n",
        "    x3 = SeparableConv2D(728, (7, 7), padding='same',use_bias=False)(x)\n",
        "    x3 = BatchNormalization(axis=channel_axis)(x3)\n",
        "    x3 = Activation('relu')(x3)\n",
        "\n",
        "    x3 = SeparableConv2D(728, (7, 7), padding='same',use_bias=False)(x3)\n",
        "    x3 = BatchNormalization(axis=channel_axis)(x3)\n",
        "    x3= Activation('relu')(x3)\n",
        "    \n",
        "    x3 = SeparableConv2D(728, (7, 7), padding='same',use_bias=False)(x3)\n",
        "    x3 = BatchNormalization(axis=channel_axis)(x3)\n",
        "    #no relu\n",
        "    #no maxpool\n",
        "    \n",
        "    #ADD\n",
        "    x = add([x1, x2, x3, residual])\n",
        "    \n",
        "###################################################################################################\n",
        "\n",
        "##\n",
        "residual = Conv2D(1024, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
        "residual = BatchNormalization(axis=channel_axis)(residual)\n",
        "\n",
        "#\n",
        "x = Activation('relu')(x)\n",
        "x = SeparableConv2D(728, (3, 3),padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation('relu', name='block13_sepconv2_act')(x)\n",
        "\n",
        "x = SeparableConv2D(1024, (3, 3),padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "\n",
        "x = MaxPooling2D((3, 3),strides=(2, 2),padding='same')(x)\n",
        "\n",
        "#ADD\n",
        "x = add([x, residual])\n",
        "\n",
        "#\n",
        "x = SeparableConv2D(1536, (3, 3),padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "x = SeparableConv2D(2048, (3, 3),padding='same',use_bias=False)(x)\n",
        "x = BatchNormalization(axis=channel_axis)(x)\n",
        "x = Activation('relu')(x)\n",
        "\n",
        "### \n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1211, activation='softmax')(x) #1211 classes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9aTV6kBZFEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model\n",
        "model = Model(inputs = inpt, outputs= x ) #we need to define the input and the output of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj3tuSrjZFGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model Architecture Visualization\n",
        "\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zao7-5QZwjq",
        "colab_type": "text"
      },
      "source": [
        "For our learning rate, we are going to use cyclical learning rate\n",
        "\n",
        "Full paper about it can be found here: https://arxiv.org/abs/1506.01186\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlcHDwkaZFIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add cycliclr\n",
        "\n",
        "\n",
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
        "    The method cycles the learning rate between two boundaries with\n",
        "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
        "    The amplitude of the cycle can be scaled on a per-iteration or \n",
        "    per-cycle basis.\n",
        "    This class has three built-in policies, as put forth in the paper.\n",
        "    \"triangular\":\n",
        "        A basic triangular cycle w/ no amplitude scaling.\n",
        "    \"triangular2\":\n",
        "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
        "    \"exp_range\":\n",
        "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
        "        cycle iteration.\n",
        "    For more detail, please see paper.\n",
        "    \n",
        "    # Example\n",
        "        ```python\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., mode='triangular')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```\n",
        "    \n",
        "    Class also supports custom scaling functions:\n",
        "        ```python\n",
        "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
        "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                                step_size=2000., scale_fn=clr_fn,\n",
        "                                scale_mode='cycle')\n",
        "            model.fit(X_train, Y_train, callbacks=[clr])\n",
        "        ```    \n",
        "    # Arguments\n",
        "        base_lr: initial learning rate which is the\n",
        "            lower boundary in the cycle.\n",
        "        max_lr: upper boundary in the cycle. Functionally,\n",
        "            it defines the cycle amplitude (max_lr - base_lr).\n",
        "            The lr at any cycle is the sum of base_lr\n",
        "            and some scaling of the amplitude; therefore \n",
        "            max_lr may not actually be reached depending on\n",
        "            scaling function.\n",
        "        step_size: number of training iterations per\n",
        "            half cycle. Authors suggest setting step_size\n",
        "            2-8 x training iterations in epoch.\n",
        "        mode: one of {triangular, triangular2, exp_range}.\n",
        "            Default 'triangular'.\n",
        "            Values correspond to policies detailed above.\n",
        "            If scale_fn is not None, this argument is ignored.\n",
        "        gamma: constant in 'exp_range' scaling function:\n",
        "            gamma**(cycle iterations)\n",
        "        scale_fn: Custom scaling policy defined by a single\n",
        "            argument lambda function, where \n",
        "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
        "            mode paramater is ignored \n",
        "        scale_mode: {'cycle', 'iterations'}.\n",
        "            Defines whether scale_fn is evaluated on \n",
        "            cycle number or cycle iterations (training\n",
        "            iterations since start of cycle). Default is 'cycle'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksC3izENZFMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clr = CyclicLR(base_lr=0.00001, max_lr=0.00005, #we input the parameters for our cyclical learning rate\n",
        "                        step_size=40000*4., mode='triangular2') #step size is the no iterations until it reaches its peak #5-6*epoch batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD7cuPe-ZFON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We initialize our callbacks\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"vception_cp.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 10,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "#reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "#                              factor = 0.2,\n",
        "#                              patience = 3,\n",
        "#                              verbose = 1,\n",
        "#                              min_delta = 0.0001)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "#callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "callbacks = [earlystop, checkpoint, clr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HULdfhZqZFQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We compile our model #gets overwritten from clr\n",
        "adam = Adam(lr=0.00001) #default is 0.001\n",
        "\n",
        "model.compile(loss ='categorical_crossentropy', #needs one hot encoding\n",
        "             optimizer = adam,\n",
        "             metrics = [\"accuracy\"])\n",
        "\n",
        "#Default accuracy is Top-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_BAqAcia7nH",
        "colab_type": "text"
      },
      "source": [
        "Training takes place in multiple stages, where the base learning rate of clr gets reduced \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frSCVZ8LZFSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fit  #keras has a special fit function that takes as an input the generators\n",
        " \n",
        "epochs = 50\n",
        "#batch_size = 32 look on top\n",
        "    \n",
        "r = model.fit_generator(train_generator, validation_data = valid_generator, epochs = epochs,\n",
        "                        steps_per_epoch = len(train_ids)//batch_size, #// rounds the result\n",
        "                        validation_steps = len(valid_ids)//batch_size,\n",
        "                        callbacks = callbacks, verbose = 1 \n",
        "                       )\n",
        "# r contains the results not the model itself"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZUzAlIDZFUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model saving for safety  \n",
        "\n",
        "model.save(\"vception_1.h5\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xeqfL0nZFWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clr = CyclicLR(base_lr=0.000001, max_lr=0.00001,\n",
        "                        step_size=40000*4., mode='triangular2') #step size is the no iterations until it reaches its peak #5-6*epoch batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vPOLcvlZFY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fit  #keras has a special fit function that takes as an input the generators\n",
        " \n",
        "epochs = 20\n",
        "#batch_size = 32 look on top\n",
        "    \n",
        "r2 = model.fit_generator(train_generator, validation_data = valid_generator, epochs = epochs,\n",
        "                        steps_per_epoch = len(train_ids)//batch_size, #// rounds the result\n",
        "                        validation_steps = len(valid_ids)//batch_size,\n",
        "                        callbacks = callbacks, verbose = 1 \n",
        "                       )\n",
        "# r contains the results not the model itself   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QTZVNBuXoEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model saving for safety  \n",
        "\n",
        "model.save(\"vception_2.h5\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvwjKMbWXoGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFy0vX_6XoIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAu8tAHiXoK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qWtK3fQXoNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD35NdbeXoPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42FXMXmLXoRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_efvbJOXoT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}